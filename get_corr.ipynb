{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "human = pd.read_csv('./third_party/data/mturk_result/Batch_results_develop.csv')\n",
    "human = human.loc[human['ApprovalTime'].dropna().index].reset_index(drop=True)\n",
    "human = human[['Input.dataset', 'Input.task', 'Input.Speaker', 'Answer.Appropriateness_llama7b', 'Answer.Appropriateness_llama13b', 'Answer.Appropriateness_vicuna7b', 'Answer.Appropriateness_vicuna13b', 'Answer.Style_llama7b', 'Answer.Style_llama13b', 'Answer.Style_vicuna7b', 'Answer.Style_vicuna13b']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Answer.Appropriateness_llama7b      51.13\n",
       "Answer.Appropriateness_llama13b     54.25\n",
       "Answer.Appropriateness_vicuna7b     59.49\n",
       "Answer.Appropriateness_vicuna13b    71.08\n",
       "Answer.Style_llama7b                66.03\n",
       "Answer.Style_llama13b               68.31\n",
       "Answer.Style_vicuna7b               66.53\n",
       "Answer.Style_vicuna13b              62.46\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human[human['Input.task']=='Negative'][['Answer.Appropriateness_llama7b', 'Answer.Appropriateness_llama13b', 'Answer.Appropriateness_vicuna7b', 'Answer.Appropriateness_vicuna13b', 'Answer.Style_llama7b', 'Answer.Style_llama13b', 'Answer.Style_vicuna7b', 'Answer.Style_vicuna13b']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./third_party/output_dir_corr/DD_Formal/response_form_result_DD_5shot_llama7b/test.1.json') as user_file:\n",
    "    parsed_json = json.load(user_file)\n",
    "data = parsed_json['instances'][0:50]\n",
    "DD_input_li = [item['input'] for item in data]\n",
    "\n",
    "with open('./third_party/output_dir_corr/BST_Formal/response_form_result_BST_5shot_llama7b/test.1.json') as user_file:\n",
    "    parsed_json = json.load(user_file)\n",
    "data = parsed_json['instances'][0:50]\n",
    "BST_input_li = [item['input'] for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_li = ['DD', 'BST']\n",
    "task_li = ['Formal', 'Informal', 'Positive', 'Negative']\n",
    "model_li = ['llama7b','llama13b','vicuna7b','vicuna13b']\n",
    "# auto_li = ['NLL','BLEU','SBERT','Style Strength']\n",
    "auto_li = ['NLL','ChatGPT','BLEU','SBERT','Style Strength']\n",
    "# appro_auto_li = ['NLL','BLEU','SBERT']\n",
    "appro_auto_li = ['NLL','ChatGPT','BLEU','SBERT']\n",
    "# style_auto_li = ['Style Strength']\n",
    "style_auto_li = ['Style Strength']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = './third_party/output_dir_corr'\n",
    "\n",
    "\n",
    "df_li = []\n",
    "for dataset in dataset_li:\n",
    "    for task in task_li:\n",
    "        dir_path = root_path+'/'+dataset+'_'+task\n",
    "\n",
    "        input_li = DD_input_li if dataset=='DD' else BST_input_li\n",
    "        auto_df = pd.DataFrame(index=input_li)\n",
    "        auto_df.index.name = 'Input.Speaker'\n",
    "        for auto in auto_li:\n",
    "            for model in model_li:\n",
    "                auto_df[f'Auto.{auto}_{model}'] = np.nan\n",
    "                \n",
    "        for file_name in os.listdir(dir_path):\n",
    "            model = file_name.split('_')[-1]\n",
    "            file_path = dir_path + '/' + file_name + '/test.1.json'\n",
    "            with open(file_path) as user_file:\n",
    "                parsed_json = json.load(user_file)\n",
    "            data_li = parsed_json['instances'][0:50]\n",
    "            for data in data_li:\n",
    "                add_data = []\n",
    "                for auto in auto_li:\n",
    "                    add_data.append(data[auto])\n",
    "                auto_df.loc[data['input'],[f'Auto.{auto}_{model}' for auto in auto_li]] = add_data\n",
    "        auto_df = auto_df.reset_index()\n",
    "        auto_df['Input.dataset'] = dataset\n",
    "        auto_df['Input.task'] = task  \n",
    "        df_li.append(auto_df)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df_total = pd.concat(df_li)\n",
    "auto_df_total['Input.Speaker'] = auto_df_total['Input.Speaker'].apply(lambda x: x.strip())\n",
    "human['Input.Speaker'] = human['Input.Speaker'].apply(lambda x: x.strip())\n",
    "data_total = pd.merge(human, auto_df_total, how='left', on=['Input.task','Input.Speaker','Input.dataset'])\n",
    "data_total_bloom = data_total.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample level\n",
      "Pearson Correlation\n",
      "\n",
      "task:  Formal\n",
      "NLL:  34.233441246299265\n",
      "ChatGPT:  27.042071644474895\n",
      "BLEU:  -11.363660021965126\n",
      "SBERT:  11.274251997683718\n",
      "Style Strength:  36.57993489868076\n",
      "\n",
      "\n",
      "task:  Informal\n",
      "NLL:  11.572363040780909\n",
      "ChatGPT:  21.32424398022153\n",
      "BLEU:  1.5084893375040096\n",
      "SBERT:  8.216736983094501\n",
      "Style Strength:  22.471223101388873\n",
      "\n",
      "\n",
      "task:  Positive\n",
      "NLL:  27.478064038983145\n",
      "ChatGPT:  28.928151724795114\n",
      "BLEU:  11.38463514144536\n",
      "SBERT:  20.01424738232686\n",
      "Style Strength:  16.63982702751144\n",
      "\n",
      "\n",
      "task:  Negative\n",
      "NLL:  42.05501922357687\n",
      "ChatGPT:  15.234138082058072\n",
      "BLEU:  5.104175245314683\n",
      "SBERT:  28.665750094310464\n",
      "Style Strength:  32.106553670387115\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print('Sample level')\n",
    "print('Pearson Correlation\\n')\n",
    "\n",
    "\n",
    "for task in task_li:\n",
    "    print('task: ', task)\n",
    "    \n",
    "    data_tmp = data_total[(data_total['Input.task']==task)]\n",
    "    data_tmp_human = data_tmp[[f'Answer.Appropriateness_{model}' for model in model_li]]\n",
    "    data_tmp_human_style = data_tmp[[f'Answer.Style_{model}' for model in model_li]]\n",
    "    for auto in appro_auto_li:\n",
    "        data_tmp_auto = data_tmp[[f'Auto.{auto}_{model}' for model in model_li]]\n",
    "        corr_li = []\n",
    "        for i in data_tmp_human.index:\n",
    "            corr_li.append(np.corrcoef(data_tmp_human.loc[i].values, data_tmp_auto.loc[i].values)[0,-1])\n",
    "\n",
    "        print(f'{auto}: ', np.array(corr_li)[~np.isnan(corr_li)].mean()*100)\n",
    "\n",
    "    for auto in style_auto_li:\n",
    "        data_tmp_auto_style = data_tmp[[f'Auto.{auto}_{model}' for model in model_li]]\n",
    "        corr_li = []\n",
    "        for i in data_tmp_human_style.index:\n",
    "            corr_li.append(np.corrcoef(data_tmp_human_style.loc[i].values, data_tmp_auto_style.loc[i].values)[0,-1])\n",
    "        print(f'{auto}: ', np.array(corr_li)[~np.isnan(corr_li)].mean()*100)\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample level\n",
      "Kendall Correlation\n",
      "\n",
      "task:  Formal\n",
      "NLL:  25.263139397475072\n",
      "ChatGPT:  20.717839009927143\n",
      "BLEU:  -6.08204072994886\n",
      "SBERT:  7.327648540982666\n",
      "Style Strength:  32.18723414973005\n",
      "\n",
      "\n",
      "task:  Informal\n",
      "NLL:  18.973762571576177\n",
      "ChatGPT:  17.367521942103124\n",
      "BLEU:  4.9328488025747\n",
      "SBERT:  6.224966432952758\n",
      "Style Strength:  16.751278536927096\n",
      "\n",
      "\n",
      "task:  Positive\n",
      "NLL:  28.261634470233364\n",
      "ChatGPT:  26.06305820660098\n",
      "BLEU:  7.78660347623573\n",
      "SBERT:  20.17197400550434\n",
      "Style Strength:  17.800893814396186\n",
      "\n",
      "\n",
      "task:  Negative\n",
      "NLL:  34.0884155340145\n",
      "ChatGPT:  16.44967746189785\n",
      "BLEU:  3.1249875822516313\n",
      "SBERT:  26.41805636871669\n",
      "Style Strength:  19.137031629735823\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scipy import stats\n",
    "print('Sample level')\n",
    "print('Kendall Correlation\\n')\n",
    "\n",
    "for task in task_li:\n",
    "    print('task: ', task)\n",
    "    \n",
    "    data_tmp = data_total[(data_total['Input.task']==task)]\n",
    "    \n",
    "    data_tmp_human = data_tmp[[f'Answer.Appropriateness_{model}' for model in model_li]]\n",
    "    data_tmp_human_style = data_tmp[[f'Answer.Style_{model}' for model in model_li]]\n",
    "    for auto in appro_auto_li:\n",
    "        data_tmp_auto = data_tmp[[f'Auto.{auto}_{model}' for model in model_li]]\n",
    "        corr_li = []\n",
    "        for i in data_tmp_human.index:\n",
    "            corr_li.append(stats.kendalltau(data_tmp_human.loc[i].values, data_tmp_auto.loc[i].values).correlation)\n",
    "\n",
    "        print(f'{auto}: ', np.array(corr_li)[~np.isnan(corr_li)].mean()*100)\n",
    "\n",
    "    for auto in style_auto_li:\n",
    "        data_tmp_auto_style = data_tmp[[f'Auto.{auto}_{model}' for model in model_li]]\n",
    "        corr_li = []\n",
    "        for i in data_tmp_human.index:\n",
    "            corr_li.append(stats.kendalltau(data_tmp_human_style.loc[i].values, data_tmp_auto_style.loc[i].values).correlation)\n",
    "\n",
    "        print(f'{auto}: ', np.array(corr_li)[~np.isnan(corr_li)].mean()*100)\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
